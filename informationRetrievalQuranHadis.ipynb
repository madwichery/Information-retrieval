!pip install ipython-autotime
from google.colab import drive
drive.mount('/content/drive')

%load_ext autotime
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/SKRIPSI/data teh yulia/data_hadis.csv')
for no, kitab, teks in zip(df['id'], df['kitab'], df['terjemah']):
  #f = open('/content/drive/MyDrive/SKRIPSI/data teh yulia/hadis/{}-{}.txt'.format(kitab, no), 'w')
  f = open('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/stories/hadis/{}-{}.txt'.format(kitab, no), 'w')
  f.write(teks)
  f.close()

import os
jumlah_hadis = len(os.listdir('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/stories/hadis'))
print("Jumlah hadis yang terbaca:", jumlah_hadis)

%load_ext autotime
!pip install num2words
!pip install Sastrawi
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from collections import Counter
from num2words import num2words

import nltk
import os
import string
import numpy as np
import copy
import pandas as pd
import pickle
import re
import math
import time

nltk.download('stopwords')
nltk.download('punkt')
title = "stories"
alpha = 0.3
str(os.getcwd())+'/'+title+'/'

folders = [x[0] for x in os.walk('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/stories')]
#folders[0] = folders[0][:len(folders[0])]
folders = folders[1:][:-1]
folders
def print_doc(id):
    print(dataset[id])
    file = open(dataset[id][0], 'r', encoding='cp1250')
    text = file.read().strip()
    file.close()
    print(text)

dataset = []

c = False
for folder in folders:
  files = list(os.listdir(folder))
  for f in files:
    dataset.append([folder +"/"+ f, f[:-4]])

N = len(dataset)
print("Total data quran dan hadis:", N)
def convert_lower_case(data):
    return np.char.lower(data)

def remove_stop_words(data):
    stop_words = stopwords.words('indonesian')
    words = word_tokenize(str(data))
    new_text = ""
    for w in words:
        if w not in stop_words and len(w) > 1:
            new_text = new_text + " " + w
    return new_text

def remove_punctuation(data):
    symbols = "!\"#$%&()*+-./:;<=>?@[\]^_`{|}~\n"
    for i in range(len(symbols)):
        data = np.char.replace(data, symbols[i], ' ')
        data = np.char.replace(data, "  ", " ")
    data = np.char.replace(data, ',', '')
    return data

def remove_apostrophe(data):
    return np.char.replace(data, "'", "")

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(data):
    #stemmer= PorterStemmer()
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    
    tokens = word_tokenize(str(data))
    new_text = ""
    for w in tokens:
        new_text = new_text + " " + stemmer.stem(w)
    return new_text

def convert_numbers(data):
    tokens = word_tokenize(str(data))
    new_text = ""
    for w in tokens:
        try:
            w = num2words(int(w))
        except:
            a = 0
        new_text = new_text + " " + w
    new_text = np.char.replace(new_text, "-", " ")
    return new_text

def preprocess(data):
    data = convert_lower_case(data)
    data = remove_punctuation(data) #remove comma seperately
    data = remove_apostrophe(data)
    data = remove_stop_words(data)
    data = convert_numbers(data)
    data = stemming(data)
    data = remove_punctuation(data)
    data = convert_numbers(data)
    data = stemming(data) #needed again as we need to stem the words
    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one
    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one
    return data
processed_text = []
processed_title = []
processed_text = []
processed_title = []

for i in dataset[:N]:
    mulai = time.time()
    file = open(i[0], 'r', encoding="utf8", errors='ignore')
    text = file.read().strip()
    file.close()

    processed_text.append(word_tokenize(str(preprocess(text))))
    processed_title.append(word_tokenize(str(preprocess(i[1]))))
    print("waktu preprocess teks %s: %2.f detik" % (i[1], float(time.time() - mulai)))
import pickle

mode_file = "load" #@param ["save", "load", "PILIH save ATAU load CUY"]

if mode_file == 'save':
  #file1 = open('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/processed_text_full_data.txt', mode = 'wb')
  #file2 = open('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/processed_title_full_data.txt', mode = 'wb')
  #pickle.dump(processed_text, file1)
  #pickle.dump(processed_title, file2)
  #file1.close()
  #file2.close()
  print("Berhasil Disimpan")

elif mode_file == 'load':
  with open('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/processed_text_full_data.txt', mode = 'rb') as f1:
    processed_text = pickle.load(f1)
  with open('/content/drive/MyDrive/2. TF-IDF Ranking - Cosine Similarity VSM, Matching Score Boolean Model/processed_title_full_data.txt', mode = 'rb') as f2:
    processed_title = pickle.load(f2)
    
else:
  print("GA NGESAVE DAN GA NGELOAD")
DF = {}

for i in range(N):
    tokens = processed_text[i]
    for w in tokens:
        try:
            DF[w].add(i)
        except:
            DF[w] = {i}

    tokens = processed_title[i]
    for w in tokens:
        try:
            DF[w].add(i)
        except:
            DF[w] = {i}
for i in DF:
    DF[i] = len(DF[i])

print(len(DF))
total_vocab_size = len(DF)
print("Total vocab size:", total_vocab_size)

total_vocab = [x for x in DF]
print(total_vocab[:20])

def doc_freq(word):
    c = 0
    try:
        c = DF[word]
    except:
        pass
    return c
doc = 0

tf_idf = {}

for i in range(N):
    
    tokens = processed_text[i]
    
    counter = Counter(tokens + processed_title[i])
    words_count = len(tokens + processed_title[i])
    
    for token in np.unique(tokens): 
        
        tf = counter[token]/words_count
        df = doc_freq(token)
        #idf = np.log((N+1)/(df+1))
        idf = np.log((N)/(df))
        
        tf_idf[doc, token] = tf*idf

    doc += 1
doc = 0

tf_idf_title = {}

for i in range(N):
    
    tokens = processed_title[i]
    counter = Counter(tokens + processed_text[i])
    words_count = len(tokens + processed_text[i])

    for token in np.unique(tokens):
        
        tf = counter[token]/words_count
        df = doc_freq(token)
        idf = np.log((N)/(df))
        
        tf_idf_title[doc, token] = tf*idf

    doc += 1
for i in tf_idf:
    tf_idf[i] *= alpha
for i in tf_idf_title:
    tf_idf[i] = tf_idf_title[i]
len(tf_idf)
jumlah_dokumen = 10 #@param {type: 'integer'}
query = "BuKu MerAH PuTIh\"" #@param {type: 'string'}
pake_hashing = 'false' #@param ['true', 'false']

def matching_score(k, query, pake_hashing):
    global hashing
    preprocessed_query = preprocess(query)
    tokens = word_tokenize(str(preprocessed_query))

    print("Matching Score / Boolean Model")
    print("\nQuery:", query)
    print("")
    print(tokens)
    
    query_weights = {}

    if pake_hashing == 'false':
      # Gapake Hashing
      for key in tf_idf:
          if key[1] in tokens:
              try:
                  query_weights[key[0]] += tf_idf[key]
                  print("Try {} kata: {}".format(key, tf_idf[key]))
              except:
                  query_weights[key[0]] = tf_idf[key]
    else:
      # Pake Hashing
      for kata in tokens:
        try:
          for i in hashing[kata]:
            if i not in query_weights:
              query_weights[i] = tf_idf[(i, kata)]
            else:
              query_weights[i] += tf_idf[(i, kata)]
        except:
          print('Query', kata, 'Tidak Ditemukan')

    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)
    print(query_weights)
    print("")
    
    l = []
    
    print("hasil pencarian dari queri, bobot terbesar adalah teks ke:")
    for i in query_weights[:k]:
        l.append(i[0])
    
    print(l)
    

matching_score(jumlah_dokumen, query, pake_hashing)
def matching_score(k, query):
    preprocessed_query = preprocess(query)
    tokens = word_tokenize(str(preprocessed_query))

    print("Matching Score / Boolean Model")
    print("\nQuery:", query)
    print("")
    print(tokens)
    
    query_weights = {}

    for key in tf_idf:
        
        if key[1] in tokens:
            try:
                query_weights[key[0]] += tf_idf[key]
            except:
                query_weights[key[0]] = tf_idf[key]
    
    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)

    print("")
    
    l = []
    
    print("hasil pencarian dari queri, bobot terbesar adalah teks ke:")
    for i in query_weights[:10]:
        l.append(i[0])
    
    print(l)
    

matching_score(10, "balasan-api-air")

def cosine_sim(a, b):
    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
    return cos_sim
D = np.zeros((N, total_vocab_size))
for i in tf_idf:
    try:
        ind = total_vocab.index(i[1])
        D[i[0]][ind] = tf_idf[i]
    except:
        pass
def gen_vector(tokens):

    Q = np.zeros((len(total_vocab)))
    
    counter = Counter(tokens)
    words_count = len(tokens)

    query_weights = {}
    
    for token in np.unique(tokens):
        
        tf = counter[token]/words_count
        df = doc_freq(token)
        idf = math.log((N+1)/(df+1))

        try:
            ind = total_vocab.index(token)
            Q[ind] = tf*idf
        except:
            pass
    return Q
def cosine_similarity(k, query):
    print("Cosine Similarity / Vector Based Model")
    preprocessed_query = preprocess(query)
    tokens = word_tokenize(str(preprocessed_query))
    
    print("\nQuery:", query)
    print("")
    print(tokens)
    
    d_cosines = []
    
    query_vector = gen_vector(tokens)
    
    for d in D:
        d_cosines.append(cosine_sim(query_vector, d))

    test1 = np.array(d_cosines)
    print(test1)  
    out = np.array(d_cosines).argsort()[-k:][::-1]
    
    print("")
    
    print("hasil pencarian dari queri, bobot terbesar adalah teks ke:")
    print(out)

#     for i in out:
#         print(i, dataset[i][0])

Q = cosine_similarity(10, "segumpal darah kami ciptakan manusia")
